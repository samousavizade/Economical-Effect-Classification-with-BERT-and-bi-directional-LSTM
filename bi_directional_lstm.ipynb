{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from hazm import Normalizer, sent_tokenize, word_tokenize, Stemmer, Lemmatizer, POSTagger, Chunker, tree2brackets, DependencyParser, stopwords_list\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Read Json Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./dataset_annotated_finance.json\"\n",
    "\n",
    "file = open(path)\n",
    "data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1450, 75, 75)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data[\"train\"]\n",
    "validation = data[\"eval\"]\n",
    "test = data[\"test\"]\n",
    "\n",
    "text_column_name = \"Text\"\n",
    "label_column_name = \"Label\"\n",
    "\n",
    "len(train), len(validation), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def majority_vote(input_list):\n",
    "    # counter = {\n",
    "    #     \"positive\" : 0,\n",
    "    #     \"negative\" : 0,\n",
    "    #     \"neutral\" : 0,\n",
    "    # }\n",
    "    # for vote in input_list:\n",
    "    #     if \"مثبت\" in vote:\n",
    "    #         counter[\"positive\"] += 1\n",
    "    #     elif \"منفی\" in vote:\n",
    "    #         counter[\"negative\"] += 1\n",
    "    #     else:\n",
    "    #         counter[\"neutral\"] += 1\n",
    "    #\n",
    "    # return max(counter, key=counter.get)\n",
    "    return Counter(input_list).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "N = 2\n",
    "train = filter(lambda sample: len(set(sample[\"annotations\"])) <= N, train)\n",
    "validation = filter(lambda sample: len(set(sample[\"annotations\"])) <= N, validation)\n",
    "test = filter(lambda sample: len(set(sample[\"annotations\"])) <= N, test)\n",
    "\n",
    "train = pd.DataFrame([[sample[\"text\"], majority_vote(sample[\"annotations\"])] for sample in train], columns=[text_column_name, label_column_name])\n",
    "validation = pd.DataFrame([[sample[\"text\"], majority_vote(sample[\"annotations\"])] for sample in validation], columns=[text_column_name, label_column_name])\n",
    "test = pd.DataFrame([[sample[\"text\"], majority_vote(sample[\"annotations\"])] for sample in test], columns=[text_column_name, label_column_name])\n",
    "\n",
    "categories = train[label_column_name].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Remove URLs and HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "\n",
    "\n",
    "def remove_URL(text):\n",
    "    text = str(text)\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "\n",
    "html = re.compile(r\"<.*?>\")\n",
    "\n",
    "\n",
    "def remove_HTML(text):\n",
    "    text = str(text)\n",
    "    return html.sub(r\"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    \"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\",\n",
    "    flags=re.UNICODE,\n",
    ")\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    text = str(text)\n",
    "    return emoji_pattern.sub(r\"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    text = str(text)\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return normalizer.normalize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    sws = stopwords_list().copy()\n",
    "    sws = set(sws)\n",
    "    sws = sws.union({})\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sws]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_number(text):\n",
    "    text = str(text)\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pre-processing Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_up_pipeline(data):\n",
    "    cleaning_functions = [\n",
    "        str,\n",
    "        str.strip,\n",
    "        str.lower,\n",
    "        remove_number,\n",
    "        remove_URL,\n",
    "        remove_HTML,\n",
    "        remove_emoji,\n",
    "        remove_punctuations,\n",
    "        remove_stopwords\n",
    "    ]\n",
    "\n",
    "    for function in cleaning_functions:\n",
    "        data.loc[:, text_column_name] = data[text_column_name].map(function)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "train = clean_up_pipeline(train)\n",
    "validation = clean_up_pipeline(validation)\n",
    "test = clean_up_pipeline(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convert Data Labels to Dummy Variable\n",
    "\n",
    "to compare with predicted probabilities for each category that model outputs in softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "خنثی               1060\n",
       "غیر مستقیم مثبت     146\n",
       "غیر مستقیم منفی      91\n",
       "مستقیم مثبت          48\n",
       "مستقیم منفی          34\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[label_column_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "خنثی               47\n",
       "غیر مستقیم مثبت     9\n",
       "غیر مستقیم منفی     8\n",
       "مستقیم مثبت         7\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation[label_column_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "خنثی               50\n",
       "غیر مستقیم مثبت    10\n",
       "غیر مستقیم منفی     6\n",
       "مستقیم مثبت         2\n",
       "مستقیم منفی         2\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[label_column_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = pd.get_dummies(train[label_column_name].astype(pd.CategoricalDtype(categories=categories)))\n",
    "validation_labels = pd.get_dummies(validation[label_column_name].astype(pd.CategoricalDtype(categories=categories)))\n",
    "test_labels = pd.get_dummies(test[label_column_name].astype(pd.CategoricalDtype(categories=categories)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build Train, Validation and Test Dataset\n",
    "build train, validation and test dataset tensorflow object from modified texts and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "MAX_VOCAB_SIZE = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train[text_column_name].values, train_labels.values)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation[text_column_name].values, validation_labels.values)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Use Pre-trained FastText Embedding Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "download farsi words embedding vectors .bin from [FastText Repository](https://fasttext.cc/docs/en/crawl-vectors.html) and place in working directory. (for persian its 'cc.fa.300.bin'), alternatively mentioned .bin embedding vectors file can be downloaded with the following command (first change directory to directory that fasttext is installed):\n",
    "``\n",
    "./download_model.py fa     # Farsi\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load FastText Model\n",
    "\n",
    "load fasttext model .bin file from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from fasttext.util import reduce_model\n",
    "\n",
    "# ft = fasttext.load_model('cc.fa.300.bin', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build Embedding Matrix\n",
    "\n",
    "build embedding matrix using pre-trained fasttext embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = tf.keras.layers.TextVectorization(max_tokens=MAX_VOCAB_SIZE)\n",
    "vectorizer.adapt(train[text_column_name].values)\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "\n",
    "EMBEDDING_SIZE = ft.get_dimension()\n",
    "\n",
    "E = np.zeros((len(vocabulary), EMBEDDING_SIZE))\n",
    "for i, word in enumerate(vocabulary):\n",
    "    E[i] = ft.get_word_vector(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build Embedding Layer\n",
    "\n",
    "build embedding layer with the help of embedding matrix as initialize state. (computed in the previous step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    len(vocabulary), EMBEDDING_SIZE,\n",
    "    embeddings_initializer=Constant(E),\n",
    "    trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Model Architecture\n",
    "\n",
    "We use bidirectional lstm layer in this project, because of the text has no order.\n",
    "\n",
    "Bidirectional long-short term memory(Bidirectional LSTM) is the process of making any neural network to have the sequence information in both directions backwards (future to past) or forward(past to future). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_20 (Text  (None, None)             0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_22 (Embedding)    (None, None, 300)         4787400   \n",
      "                                                                 \n",
      " bidirectional_18 (Bidirecti  (None, 600)              1442400   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 300)               180300    \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 5)                 1505      \n",
      "                                                                 \n",
      " softmax_18 (Softmax)        (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,411,605\n",
      "Trainable params: 6,411,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    vectorizer,\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(EMBEDDING_SIZE)),\n",
    "    tf.keras.layers.Dense(EMBEDDING_SIZE, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(5),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because of imbalanced weights of categories in label column, we use AUC (Area under the Curve and F1) & F1-Score as metrics for model performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def F1(y_true, y_pred):\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision*recall) / (precision + recall + K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.metrics import CategoricalAccuracy, AUC\n",
    "from keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam, RMSprop, Nadam\n",
    "\n",
    "model.compile(loss=CategoricalCrossentropy(from_logits=True, name=\"CCELoss\"),\n",
    "              optimizer=Nadam(learning_rate=1e-4),\n",
    "              metrics=[CategoricalAccuracy(name=\"Accuracy\"), AUC(name=\"AUC\"), F1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Class Weights\n",
    "\n",
    "weights of categories in training dataset are totally imbalanced therefore it is necessary to set class weight in training phase.\n",
    "\n",
    "The 'balanced' heuristic is inspired by Logistic Regression in Rare Events Data, King, Zen, 2001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "labels_codes = np.argmax(train_labels.values, axis=1)\n",
    "weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                            classes=np.unique(labels_codes),\n",
    "                                            y=labels_codes)\n",
    "\n",
    "class_weights = {}\n",
    "for class_index, weight in enumerate(weights):\n",
    "    class_weights[class_index] = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train, Validation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 22s 875ms/step - loss: 1.6089 - Accuracy: 0.3495 - AUC: 0.7400 - f1_m: 0.0000e+00 - val_loss: 1.6070 - val_Accuracy: 0.3662 - val_AUC: 0.7252 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 19s 843ms/step - loss: 1.6059 - Accuracy: 0.4859 - AUC: 0.8116 - f1_m: 0.0000e+00 - val_loss: 1.6059 - val_Accuracy: 0.4366 - val_AUC: 0.7101 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 19s 851ms/step - loss: 1.6005 - Accuracy: 0.6178 - AUC: 0.8456 - f1_m: 0.0000e+00 - val_loss: 1.6029 - val_Accuracy: 0.3521 - val_AUC: 0.6971 - val_f1_m: 0.0000e+00\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 18s 848ms/step - loss: 1.5720 - Accuracy: 0.6759 - AUC: 0.8210 - f1_m: 0.0617 - val_loss: 1.4990 - val_Accuracy: 0.6620 - val_AUC: 0.8776 - val_f1_m: 0.0299\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 18s 809ms/step - loss: 1.4983 - Accuracy: 0.7375 - AUC: 0.8937 - f1_m: 0.6071 - val_loss: 1.3924 - val_Accuracy: 0.5915 - val_AUC: 0.8526 - val_f1_m: 0.5577\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 18s 850ms/step - loss: 1.4481 - Accuracy: 0.7063 - AUC: 0.9055 - f1_m: 0.6725 - val_loss: 1.4324 - val_Accuracy: 0.4507 - val_AUC: 0.7629 - val_f1_m: 0.5083\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 18s 836ms/step - loss: 1.4117 - Accuracy: 0.7063 - AUC: 0.9021 - f1_m: 0.6961 - val_loss: 1.2618 - val_Accuracy: 0.6479 - val_AUC: 0.8653 - val_f1_m: 0.7066\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 18s 845ms/step - loss: 1.3747 - Accuracy: 0.6896 - AUC: 0.9127 - f1_m: 0.7134 - val_loss: 1.3359 - val_Accuracy: 0.5775 - val_AUC: 0.8663 - val_f1_m: 0.6995\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 18s 826ms/step - loss: 1.3244 - Accuracy: 0.7194 - AUC: 0.9294 - f1_m: 0.7299 - val_loss: 1.2334 - val_Accuracy: 0.6901 - val_AUC: 0.8809 - val_f1_m: 0.7101\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 18s 838ms/step - loss: 1.2746 - Accuracy: 0.7382 - AUC: 0.9344 - f1_m: 0.7373 - val_loss: 1.2476 - val_Accuracy: 0.6761 - val_AUC: 0.8837 - val_f1_m: 0.6439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/06/22 18:49:04 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: '/home/ahur4/NLP Project 4/NLP/HW4/mlruns' does not exist.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.tensorflow.autolog(log_models=False)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    class_weight=class_weights,\n",
    "    epochs=10,\n",
    "    workers=4,\n",
    "    use_multiprocessing=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 75ms/step - loss: 1.2480 - Accuracy: 0.6571 - AUC: 0.8765 - f1_m: 0.6249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2480436563491821,\n",
       " 0.6571428775787354,\n",
       " 0.8764795660972595,\n",
       " 0.6249104142189026]"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test[text_column_name].values, test_labels.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 90ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_probabilities_test_labels = model.predict(validation[text_column_name].values)\n",
    "predicted_test_labels = np.argmax(predicted_probabilities_test_labels, axis=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34daa296ffe99e8a66e159d01b1dfeb9a87967b5cca691fda43c054f03617153"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
